# Reading Notes

## Paper notes

### Large Language Model

* [8-Bit Optimizers via Block-wise Quantization](./papers/LLM/8BitBlockwiseQuantization.md) [[paper](https://arxiv.org/abs/2110.02861v1)][[code](https://github.com/TimDettmers/bitsandbytes)]

* [Attention Is All You Need](./papers/LLM/AttentionIsAllYouNeed.md) [[paper](https://arxiv.org/abs/1706.03762)]

* [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](./papers/LLM/FlashAttention.md) [[paper](https://arxiv.org/abs/2205.14135)] [[code](https://github.com/Dao-AILab/flash-attention)]

* [LLAMA 2: Open Foundation and Fine-Tuned Chat Models](./papers/LLM/LLAMA2.md) [[paper](https://arxiv.org/abs/2307.09288)][[code](https://github.com/facebookresearch/llama)]

* TODO [Textbooks Are All You Need II: phi-1.5 technical report](./papers/LLM/Phi1.5.md) [[paper](https://arxiv.org/abs/2309.05463)]

#### Knowledge distillation

* [Zephyr: Direct Distillation of LM Alignment](./papers/LLM/distillation/Zephyr.md) [[paper](https://arxiv.org/abs/2310.16944)]

## Blog notes

* [GPU Performance Background User's Guide](./blogs/GPUPerformanceBackgroundUserGuide.md) [[blog](https://docs.nvidia.com/deeplearning/performance/dl-performance-gpu-background/index.html#undefined)]

* [How continuous batching enables 23x throughput in LLM inference while reducing p50 latency](./blogs/ContinuousBatch.md) [[blog](https://www.anyscale.com/blog/continuous-batching-llm-inference)][[code](https://github.com/anyscale/llm-continuous-batching-benchmarks)]

* [Numbers every LLM Developer should know](./blogs/NumbersEveryLLMDeveloperShouldKnow.md) [[blog](https://github.com/ray-project/llm-numbers)]

* TODO [Transformer Inference Arithmetic](./blogs/TransformerInferenceArithmetic.md) [[blog](https://kipp.ly/transformer-inference-arithmetic/#kv-cache)]

* TODO [Ray](./blogs/ray/)

## Book notes

* [thinking in cpp](./books/thinking_in_cpp/)
