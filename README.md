# Reading Notes

## Paper notes

### Large Language Model

* [8-Bit Optimizers via Block-wise Quantization](./papers/LLM/8BitBlockwiseQuantization.md) [[paper](https://arxiv.org/abs/2110.02861v1)][[code](https://github.com/TimDettmers/bitsandbytes)]

* [Attention Is All You Need](./papers/LLM/AttentionIsAllYouNeed.md) [[paper](https://arxiv.org/abs/1706.03762)]

* [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](./papers/LLM/FlashAttention.md) [[paper](https://arxiv.org/abs/2205.14135)] [[code](https://github.com/Dao-AILab/flash-attention)]

* [LLAMA 2: Open Foundation and Fine-Tuned Chat Models](./papers/LLM/LLAMA2.md) [[paper](https://arxiv.org/abs/2307.09288)][[code](https://github.com/facebookresearch/llama)]

* TODO [Textbooks Are All You Need II: phi-1.5 technical report](./papers/LLM/Phi1.5.md) [[paper](https://arxiv.org/abs/2309.05463)]

#### Knowledge distillation

* [Distilling Step-by-Step! Outperforming Larger Lauguage Models with Less Training Data and Smaller Model Sizes](./papers/LLM/distillation/DistillingStepByStep.md) [[paper](https://arxiv.org/abs/2305.02301)] [[code](https://github.com/google-research/distilling-step-by-step)]

* [LaMini-LM: A Diverse Herd of Distilled Models from Large-Scale Instructions](./papers/LLM/distillation/LaMini.md) [[paper](https://arxiv.org/abs/2304.14402)]

* [Specializing Smaller Language Models towards Multi-Step Reasoning](./papers/LLM/distillation/SpecializedSLM.md) [[paper](https://arxiv.org/abs/2301.12726)] [[code](https://github.com/FranxYao/FlanT5-CoT-Specialization)]

* [UniversalNER: Targeted Distillation from Large Language Models for Open Named Entity Recognition](./papers/LLM/distillation/UniversalNER.md) [[paper](https://arxiv.org/pdf/2308.03279)] [[code](https://github.com/universal-ner/universal-ner)]

* [Zephyr: Direct Distillation of LM Alignment](./papers/LLM/distillation/Zephyr.md) [[paper](https://arxiv.org/abs/2310.16944)] [[code](https://github.com/mbzuai-nlp/LaMini-LM)]

## Blog notes

* [GPU Performance Background User's Guide](./blogs/GPUPerformanceBackgroundUserGuide.md) [[blog](https://docs.nvidia.com/deeplearning/performance/dl-performance-gpu-background/index.html#undefined)]

* [How continuous batching enables 23x throughput in LLM inference while reducing p50 latency](./blogs/ContinuousBatch.md) [[blog](https://www.anyscale.com/blog/continuous-batching-llm-inference)][[code](https://github.com/anyscale/llm-continuous-batching-benchmarks)]

* [Numbers every LLM Developer should know](./blogs/NumbersEveryLLMDeveloperShouldKnow.md) [[blog](https://github.com/ray-project/llm-numbers)]

* TODO [Transformer Inference Arithmetic](./blogs/TransformerInferenceArithmetic.md) [[blog](https://kipp.ly/transformer-inference-arithmetic/#kv-cache)]

* TODO [Ray](./blogs/ray/)

## Book notes

* [thinking in cpp](./books/thinking_in_cpp/)
