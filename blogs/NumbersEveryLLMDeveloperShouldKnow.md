# [Numbers every LLM Developer should know](https://github.com/ray-project/llm-numbers#1-mb-gpu-memory-required-for-1-token-of-output-with-a-13b-parameter-model)

## 提示

- **40-90%** 通过在提示符后面加上“简明扼要”节省的金额

  重要的是要记住，您按token支付。这意味着要求LLM简明扼要可以为你节省很多钱。这一点可以扩展，而不仅仅是在你的提示后面加上“简明扼要”：如果你使用GPT-4来提出10个选择，也许可以要求它提供5个，并保留另一半钱。

- **1.3:1** 每个单词的平均token

  LLM在token上操作。Token是单词或单词的子部分，所以“eating”可以分为两个token“eat”和“ing”。一份750字的英文文档大约需要1000个token。对于英语以外的语言，每个单词的token增加取决于它们在LLM嵌入语料库中的通用性。

## 价格

价格当然会发生变化，但考虑到LLM的运营成本，这部分的数字至关重要。我们使用OpenAI来计算这里的数字，但你应该看看其他供应商(Anthropic, Cohere)的价格。

- **~50:1** GPT-4与GPT-3.5 Turbo的成本比

  这意味着，对于许多实际应用程序，最好使用GPT-4来生成高质量的微调数据，或者对其他模型进行自动评估——这些事情可能只需要执行一次，而不是在推理周期中使用它。使用GPT-3.5-Turbo比使用GPT-4便宜大约50倍(“大约”是因为GPT-4对提示和生成输出的收费不同)，所以你真的需要检查使用GPT-3.5-Turbo能走多远。GPT-3.5-Turbo对于诸如总结之类的任务来说绰绰有余。

- **5:1** 使用GPT-3.5-Turbo与OpenAI嵌入生成文本的成本比

  这意味着在矢量存储中查找内容要比让LLM生成它便宜得多。“特拉华州的首府是什么?”“当在神经信息检索系统中查找时，成本比使用GPT-3.5-Turbo要低5倍。与GPT-4相比，成本差异是惊人的250倍!

- **10:1** OpenAI嵌入与自托管嵌入的成本比

  在我们的博客文章中，我们注意到使用g4dn.4×large(按需价格:1.20美元/小时)我们能够每秒嵌入大约9000个令牌(这几乎与OpenAI的嵌入一样好)。对该速率和节点类型进行一些基本的数学计算表明，它比自托管嵌入要便宜得多(便宜10倍)(这是在您开始考虑诸如入口和出口费用之类的事情之前)。

- **6:1** OpenAI微调与基本模型查询的成本比

  在OpenAI上提供一个微调模型的成本是基础模型的6倍。这是相当昂贵的，但可能是有意义的，因为基本模型可能有多租户。这也意味着调整基本模型的提示比调整定制模型的提示更具成本效益。

- **1:1** 自托管与微调模型查询的成本比

  如果是自托管一个模型，那么为一个微调模型提供服务的成本与为一个基本模型提供服务的成本或多或少是相同的（模型具有相同数量的参数）。

## 训练和微调

- **大约100万美元** 在1.4万亿个令牌上训练130亿个参数模型的成本

  LLaMa论文提到，他们花了21天的时间来训练LLaMa，使用2048个A100 80GB gpu。我们考虑在Red Pajama的训练集上训练我们自己的模型。以上是假设一切正常，没有崩溃，一次计算成功，等等。此外，它还涉及到2048个gpu的协调。这不是大多数公司都能做到的。重点是，训练自己的LLM是可能的，但成本并不低。每次运行都需要几天的时间。使用预训练模型要便宜得多。

- **<0.001** 微调与从头开始训练的成本比

  这有点泛化，但微调的成本可以忽略不计。例如，我们展示了你可以用7美元微调一个6B参数模型。即使以OpenAI最昂贵的可调模型达芬奇(Davinci)的价格计算，每1000个代币也只有3美分。这意味着，如果你想阅读莎士比亚的全部作品(大约100万字)，你需要花费40美元。然而，微调是一回事，从头开始训练是另一回事……

## GPU内存

如果您是自托管模型，那么了解GPU内存非常重要，因为LLM将GPU内存推向极限。下面的统计数据是专门关于推理的。您需要相当多的内存用于训练或微调。

- **V100: 16GB, A10G: 24GB, A100: 40/80GB** GPU内存容量

  这可能看起来很奇怪，但了解不同类型gpu的内存量是很重要的。这将限制您的LLM可以拥有的参数数量。一般来说，我们喜欢使用A10G，因为按照AWS按需价格，它们每小时的价格为1.5美元到2美元，并且具有24G的GPU内存，而按照AWS按需价格，A100每小时的价格约为5美元。

- **2倍参数量** LLM服务所需的典型GPU内存

  例如，如果你有一个70亿个参数模型，它需要大约14GB的GPU空间。这是因为大多数情况下，每个参数需要一个16位浮点数(或2字节)。通常不需要超过16位精度，大多数情况下，当你达到8位精度时，你会开始失去分辨率(尽管在某些情况下这是可以接受的)。当然，也有人在努力减少这种情况，特别是llama.cpp，它通过量化到4位，在6GB GPU上运行130亿个参数模型，但这是不典型的。

- **约1GB** 嵌入式模型对GPU内存的典型要求

  当你在做句子嵌入时(对于聚类、语义搜索和分类任务来说，这是一件非常典型的事情)，你需要一个像句子转换器这样的嵌入模型。OpenAI也有自己的商业嵌入。

  你通常不需要担心嵌入在GPU上占用多少内存，它们相当小。我们甚至在同一个GPU上实现了嵌入和LLM。

- **>10倍** 通过批处理LLM请求提高吞吐量

  通过GPU运行LLM查询的延迟非常高：可能需要5秒，吞吐量为每秒0.2个查询。有趣的是，如果运行两个任务，可能只需要5.2秒。这意味着如果您可以将25个查询捆绑在一起，则大约需要10秒，并且我们的吞吐量已提高到每秒2.5个查询。但是，看看下一点。

- **约1MB** 使用13B参数模型输出一个令牌所需的GPU内存

  您需要的内存量与您想要生成的token的最大数量成正比。因此，例如，如果您想生成多达512个token(大约380个单词)的输出，则需要512MB。你可能会说，没什么大不了的——我有24GB的空闲空间，512MB算什么?好吧，如果你想要运行更大的批量，它就会开始累积起来。所以如果你想做16个批次，你需要8GB的空间。有一些技术正在开发中，可以克服这一点，但这仍然是一个真正的问题。
  