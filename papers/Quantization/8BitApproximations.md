# 8-Bit Approximations for Parallelism in Deep Learning

## Motivation

创建实用的深度学习产品通常需要跨处理器和计算机并行化，使得深度学习在大型数据集上可行。并行深度学习的难点在于反向传播的序列特性，参数更新必须在下次梯度计算前全部完成。这要求GPU和计算机之间具有一个高带宽和低延迟的网络通信，网络通信的瓶颈使得通过并行性很难获得良好的加速。

## Contribution

主要存在两种方式提升并行算法的性能：

1. 同时进行计算和通信，使得大多数通信和计算同时完成。

2. 降低需要传输的数据量。

作者根据第二种方式，提出了一种8bit近似算法，主要包含以下贡献

* 讨论了模型并行和数据并行中的硬件和软件瓶颈；

* 提出的8bit近似不会影响深度学习在Mnist、Cifar10以及ImageNet上的表现，数据搬运速度提升至fp32的两倍；

* 建立了一个预测模型，预测提出的近似算法可以在96个GPU的系统上取得fp32 50倍的速度；

* 和其他类似的工作对比，8bit近似可以避开GPU集群的大batch问题，提升卷积网络的收敛速度；

* 8bit近似实现模型并行的SOTA；

## Background

### 数据并行

数据并行中，每个GPU上保存相同的模型，使用不同的 mini-batch 训练。每个iteration，同步不同GPU的梯度。

* 数据在样本维度切分，分配给不同的GPU，如对于全连接网络，1024x768的batch会被切分为4个256x768分配个四个GPU。（文中提到卷积网络存在两种切分数据的方式，样本维度切分和特征维度切分，特征维度切分不理解）

* 参数在每次前向传播和反向更新后同步。

* 数据并行在模型参数比较少时，或者每个参数的计算成本比较高时（如卷积网络）比较高效。

* 当前GPU针对大矩阵计算进行特殊优化。数据并行无法无限制的拓展，由于每个GPU只有很少的数据进行计算，小矩阵计算效率不如大矩阵。同样卷积计算也有同样的问题。batch越大，局部收敛越慢。

* 要求渐进准确率，更新序列可以渐进收敛到最小值，则可以找到好的解。

### 模型并行

模型并行中，所有GPU的数据是相同的，每个GPU保存整个模型的一部分。

* 将模型中一个层的参数沿输入或输入的维度进行分割，分配到不同的GPU上。

* 每层的参数同步一次。每层的输出需要根据运算堆叠或加到一起。

* 在模型参数量比较大时，如全连接层，模型并行有效。

* Batch大时性能差，不同GPU需要同步大矩阵。

* 要求数值准确，输出必须准确，小误差可能导致后续层产生巨大误差。

### 常见的瓶颈

* PCIE Switches

  PCI Express (PCIe) 像普通网络一样构建，其中两个 PCIe 插槽共享一个公共交换机，可以同时服务于一个传出和传入连接。因此，在任何给定时间设备对中只有一个设备可以与另一个设备通信。这适用于 GPU 和 InfiniBand 卡。因此，需要考虑PCIe交换机，以获得多GPU系统中的最佳性能。

* Bandwidth

  计算机内的 GPU 通过使用 PCIe 接口进行通信，当它包含 2 个 GPU 时，它实际上提供了大约 14 GB/s 带宽；计算机包含超过 2 个 GPU 时约 7 GB/s 带宽。

  计算机之间的 GPU 通常使用具有 3-7GB/s（对应四数据速率 QDR 和 14数据速率卡 FDR）的实际带宽的 InfiniBand 网络卡进行通信。