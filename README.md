# Reading Notes
## Paper notes
### Large Language Model
* [LLAMA 2: Open Foundation and Fine-Tuned Chat Models](./papers/LLM/LLAMA2.md) [[paper](https://arxiv.org/abs/2307.09288#:~:text=%5B2307.09288%5D%20Llama%202%3A%20Open%20Foundation%20and%20Fine-Tuned%20Chat,Llama%202%3A%20Open%20Foundation%20and%20Fine-Tuned%20Chat%20Models)][[code](https://github.com/facebookresearch/llama)]

## Blog notes
* [How continuous batching enables 23x throughput in LLM inference while reducing p50 latency](./blogs/ContinuousBatch.md) [[blog](https://www.anyscale.com/blog/continuous-batching-llm-inference)][[code](https://github.com/anyscale/llm-continuous-batching-benchmarks)]